{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wines Points prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gil LAIFER (028482636) - TCDS17 6.5.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to predict the points a wine will get based on known characteristics (i.e. features, in the ML terminology). The mine point in this stage is to establish a simple, ideally super cost effective, basline.\n",
    "In the real world there is a tradeoff between complexity and perforamnce, and the DS job, among others, is to present a tradeoff tables of what performance is achivalbel at what complexity level. \n",
    "\n",
    "to which models with increased complexity and resource demands will be compared. Complexity should then be translated into cost. For example:\n",
    " * Compute cost \n",
    " * Maintenance cost\n",
    " * Serving costs (i.e. is new platform needed?) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cufflinks as cf; cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129971, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_reviews = pd.read_csv(\"data/winemag-data-130k-v2.csv\")\n",
    "wine_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83500</th>\n",
       "      <td>83500</td>\n",
       "      <td>Italy</td>\n",
       "      <td>This is a creamy and layered wine (made with t...</td>\n",
       "      <td>Terre di Giumara</td>\n",
       "      <td>86</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Sicilia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Caruso &amp; Minini 2007 Terre di Giumara Grecanic...</td>\n",
       "      <td>Grecanico</td>\n",
       "      <td>Caruso &amp; Minini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115820</th>\n",
       "      <td>115820</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Made entirely in stainless steel, this is only...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Southern Italy</td>\n",
       "      <td>Salento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Masseria Altemura 2006 Fiano (Salento)</td>\n",
       "      <td>Fiano</td>\n",
       "      <td>Masseria Altemura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17578</th>\n",
       "      <td>17578</td>\n",
       "      <td>France</td>\n",
       "      <td>An earthy, subdued nose leads to a clean palat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anne Krebiehl MW</td>\n",
       "      <td>@AnneInVino</td>\n",
       "      <td>Domaine Charles Baur 2015 Pinot Gris (Alsace)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Domaine Charles Baur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74143</th>\n",
       "      <td>74143</td>\n",
       "      <td>Austria</td>\n",
       "      <td>A wine that shows all the typicity of cool-cli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Weinviertel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>@vossroger</td>\n",
       "      <td>Zull 2011 Grüner Veltliner (Weinviertel)</td>\n",
       "      <td>Grüner Veltliner</td>\n",
       "      <td>Zull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9671</th>\n",
       "      <td>9671</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Despite its richness, this is a stylish wine t...</td>\n",
       "      <td>Maria Mora Enamorada</td>\n",
       "      <td>90</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Alentejano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>@vossroger</td>\n",
       "      <td>Magnum Vinhos 2012 Maria Mora Enamorada Red (A...</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Magnum Vinhos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0   country  \\\n",
       "83500        83500     Italy   \n",
       "115820      115820     Italy   \n",
       "17578        17578    France   \n",
       "74143        74143   Austria   \n",
       "9671          9671  Portugal   \n",
       "\n",
       "                                              description  \\\n",
       "83500   This is a creamy and layered wine (made with t...   \n",
       "115820  Made entirely in stainless steel, this is only...   \n",
       "17578   An earthy, subdued nose leads to a clean palat...   \n",
       "74143   A wine that shows all the typicity of cool-cli...   \n",
       "9671    Despite its richness, this is a stylish wine t...   \n",
       "\n",
       "                 designation  points  price           province region_1  \\\n",
       "83500       Terre di Giumara      86   11.0  Sicily & Sardinia  Sicilia   \n",
       "115820                   NaN      87   15.0     Southern Italy  Salento   \n",
       "17578                    NaN      87   23.0             Alsace   Alsace   \n",
       "74143                    NaN      90   20.0        Weinviertel      NaN   \n",
       "9671    Maria Mora Enamorada      90  100.0         Alentejano      NaN   \n",
       "\n",
       "       region_2       taster_name taster_twitter_handle  \\\n",
       "83500       NaN               NaN                   NaN   \n",
       "115820      NaN               NaN                   NaN   \n",
       "17578       NaN  Anne Krebiehl MW           @AnneInVino   \n",
       "74143       NaN        Roger Voss            @vossroger   \n",
       "9671        NaN        Roger Voss            @vossroger   \n",
       "\n",
       "                                                    title           variety  \\\n",
       "83500   Caruso & Minini 2007 Terre di Giumara Grecanic...         Grecanico   \n",
       "115820             Masseria Altemura 2006 Fiano (Salento)             Fiano   \n",
       "17578       Domaine Charles Baur 2015 Pinot Gris (Alsace)        Pinot Gris   \n",
       "74143            Zull 2011 Grüner Veltliner (Weinviertel)  Grüner Veltliner   \n",
       "9671    Magnum Vinhos 2012 Maria Mora Enamorada Red (A...    Portuguese Red   \n",
       "\n",
       "                      winery  \n",
       "83500        Caruso & Minini  \n",
       "115820     Masseria Altemura  \n",
       "17578   Domaine Charles Baur  \n",
       "74143                   Zull  \n",
       "9671           Magnum Vinhos  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_reviews.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews = wine_reviews.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119988, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_reviews = wine_reviews.drop_duplicates()\n",
    "wine_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points is descrete value target. There for we are talking about a prediction (Regression) problem (in contrary to classification problem). Prediction solutions can be measured in few metrics:\n",
    "\n",
    "* MSE - [Mean score error](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "* R2 - [R Square](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "* MAE - [Mean absolut error](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "\n",
    "Read more [here](https://towardsdatascience.com/what-are-the-best-metrics-to-evaluate-your-regression-model-418ca481755b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test set split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly report results, let's split to train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = wine_reviews.sample(frac = 0.8)\n",
    "test_data = wine_reviews[~wine_reviews.index.isin(train_data.index)]\n",
    "assert(len(train_data) + len(test_data) == len(wine_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23998, 95990)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data), len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prediction_quality(df, pred_score_col, true_score_col):\n",
    "    return pd.Series({'MSE': mean_squared_error(df[true_score_col], df[pred_score_col]),\n",
    "                      'MAE': mean_absolute_error(df[true_score_col], df[pred_score_col]),\n",
    "                      'R2': r2_score(df[true_score_col], df[pred_score_col])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic baseline is simply the average points. The implementaion is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSE    9.550246\n",
       "MAE    2.534278\n",
       "R2    -0.000037\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['baseline_1_predicted_points'] = train_data.points.mean()\n",
    "b1_stats = calc_prediction_quality(test_data, 'baseline_1_predicted_points', 'points')\n",
    "b1_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basline 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably improve by predicting the average score based on the origin country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country\n",
       "Argentina                 86.649579\n",
       "Armenia                   88.000000\n",
       "Australia                 88.563542\n",
       "Austria                   90.152801\n",
       "Bosnia and Herzegovina    86.500000\n",
       "Name: points, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_points_by_country = train_data.groupby('country').points.mean()\n",
    "avg_points_by_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSE    9.029053\n",
       "MAE    2.459092\n",
       "R2     0.054539\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['baseline_2_predicted_points'] = test_data.country.map(avg_points_by_country).fillna(train_data.points.mean())\n",
    "b2_stats = calc_prediction_quality(test_data, 'baseline_2_predicted_points', 'points')\n",
    "b2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more breakdowns will increase our granularity but can result in overfitting. Yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country    province        \n",
       "Argentina  Mendoza Province    86.758663\n",
       "           Other               85.972152\n",
       "Armenia    Armenia             88.000000\n",
       "Australia  Australia Other     85.481081\n",
       "           New South Wales     87.681159\n",
       "Name: baseline_3_predicted_points, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_points_by_country_and_region = train_data.groupby(['country','province']).points.mean().rename('baseline_3_predicted_points')\n",
    "avg_points_by_country_and_region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23998, 16), (23998, 15))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_with_baseline_3 = test_data.merge(avg_points_by_country_and_region, on = ['country','province'], how='left')\n",
    "test_data_with_baseline_3.baseline_3_predicted_points = test_data_with_baseline_3.baseline_3_predicted_points.fillna(test_data_with_baseline_3.baseline_2_predicted_points).fillna(test_data.baseline_1_predicted_points)\n",
    "test_data_with_baseline_3.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSE    8.522805\n",
       "MAE    2.367587\n",
       "R2     0.107550\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b3_stats = calc_prediction_quality(test_data_with_baseline_3, 'baseline_3_predicted_points', 'points')\n",
    "b3_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline_1</th>\n",
       "      <td>9.550246</td>\n",
       "      <td>2.534278</td>\n",
       "      <td>-0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline_2</th>\n",
       "      <td>9.029053</td>\n",
       "      <td>2.459092</td>\n",
       "      <td>0.054539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline_3</th>\n",
       "      <td>8.522805</td>\n",
       "      <td>2.367587</td>\n",
       "      <td>0.107550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MSE       MAE        R2\n",
       "baseline_1  9.550246  2.534278 -0.000037\n",
       "baseline_2  9.029053  2.459092  0.054539\n",
       "baseline_3  8.522805  2.367587  0.107550"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_summary = pd.DataFrame([b1_stats, b2_stats, b3_stats], index=['baseline_1', 'baseline_2','baseline_3'])\n",
    "baseline_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_summary.to_csv('data/baselines_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Boosting trees regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data - Label encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['country','province','region_1','region_2','taster_name','variety','winery']\n",
    "numerical_features = ['price']\n",
    "features = categorical_features + numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>price</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>332</td>\n",
       "      <td>424</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>691</td>\n",
       "      <td>11608</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>108</td>\n",
       "      <td>738</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>451</td>\n",
       "      <td>12956</td>\n",
       "      <td>15.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>269</td>\n",
       "      <td>1218</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>437</td>\n",
       "      <td>13018</td>\n",
       "      <td>14.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>218</td>\n",
       "      <td>549</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>480</td>\n",
       "      <td>14390</td>\n",
       "      <td>13.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>269</td>\n",
       "      <td>1218</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>441</td>\n",
       "      <td>14621</td>\n",
       "      <td>65.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  province  region_1  region_2  taster_name  variety  winery  price  \\\n",
       "0       22       332       424         6            9      691   11608   -1.0   \n",
       "1       32       108       738         6           16      451   12956   15.0   \n",
       "2       41       269      1218        17           15      437   13018   14.0   \n",
       "3       41       218       549         6            0      480   14390   13.0   \n",
       "4       41       269      1218        17           15      441   14621   65.0   \n",
       "\n",
       "   points  \n",
       "0      87  \n",
       "1      87  \n",
       "2      87  \n",
       "3      87  \n",
       "4      87  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_features = wine_reviews[categorical_features].apply(lambda col: le.fit_transform(col.fillna('NA')))\n",
    "encoded_features['price'] = wine_reviews.price.fillna(-1)\n",
    "encoded_features['points'] = wine_reviews.points\n",
    "encoded_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-splitting to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_features = encoded_features[encoded_features.index.isin(train_data.index)]\n",
    "test_encoded_features = encoded_features[encoded_features.index.isin(test_data.index)]\n",
    "assert(len(train_encoded_features) + len(test_encoded_features) == len(wine_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a tree-regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import i_feel_lucky_xgboost_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 95990 entries, 0 to 129970\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   country      95990 non-null  int32  \n",
      " 1   province     95990 non-null  int32  \n",
      " 2   region_1     95990 non-null  int32  \n",
      " 3   region_2     95990 non-null  int32  \n",
      " 4   taster_name  95990 non-null  int32  \n",
      " 5   variety      95990 non-null  int32  \n",
      " 6   winery       95990 non-null  int32  \n",
      " 7   price        95990 non-null  float64\n",
      " 8   points       95990 non-null  int64  \n",
      "dtypes: float64(1), int32(7), int64(1)\n",
      "memory usage: 4.8 MB\n"
     ]
    }
   ],
   "source": [
    "train_encoded_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf, clf_name = i_feel_lucky_xgboost_training(train_encoded_features, test_encoded_features, features, 'points', name='xgb_clf_points_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the function output - specifically the **xgb_clf_points_prediction** column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>price</th>\n",
       "      <th>points</th>\n",
       "      <th>xgb_clf_points_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>269</td>\n",
       "      <td>1218</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>437</td>\n",
       "      <td>13018</td>\n",
       "      <td>14.0</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38</td>\n",
       "      <td>263</td>\n",
       "      <td>758</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>591</td>\n",
       "      <td>14706</td>\n",
       "      <td>15.0</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41</td>\n",
       "      <td>51</td>\n",
       "      <td>747</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>80</td>\n",
       "      <td>9307</td>\n",
       "      <td>19.0</td>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>41</td>\n",
       "      <td>399</td>\n",
       "      <td>1201</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>325</td>\n",
       "      <td>12968</td>\n",
       "      <td>32.0</td>\n",
       "      <td>87</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>41</td>\n",
       "      <td>269</td>\n",
       "      <td>788</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>441</td>\n",
       "      <td>135</td>\n",
       "      <td>20.0</td>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country  province  region_1  region_2  taster_name  variety  winery  \\\n",
       "2        41       269      1218        17           15      437   13018   \n",
       "5        38       263       758         6           12      591   14706   \n",
       "10       41        51       747         7           19       80    9307   \n",
       "19       41       399      1201         6            0      325   12968   \n",
       "21       41       269       788        11           15      441     135   \n",
       "\n",
       "    price  points  xgb_clf_points_prediction  \n",
       "2    14.0      87                         87  \n",
       "5    15.0      87                         87  \n",
       "10   19.0      87                         88  \n",
       "19   32.0      87                         84  \n",
       "21   20.0      87                         88  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSE    6.411826\n",
       "MAE    1.910201\n",
       "R2     0.328597\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_stats = calc_prediction_quality(test_encoded_features, 'xgb_clf_points_prediction','points')\n",
    "xgb_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared = pd.DataFrame([b1_stats, b2_stats, b3_stats, xgb_stats], index=['baseline_1', 'baseline_2','baseline_3','regression_by_xgb'])\n",
    "all_compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared.to_csv('data/all_models_compared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical NLP approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using only the text from the \"description\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import * \n",
    "import cufflinks as cf; cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_option('display.max_colwidth',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import emoji\n",
    "import re as regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialChars = ''.join([\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",'–','“', '”'\n",
    "                      \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\"!\", \"?\", \".\", \"'\",\"--\", \"---\", \"#\", '‘', '’', '…'])  \n",
    "space_chars = ['.',',',';', '&', '?','!']\n",
    "def remove_by_regex(description, regexp):\n",
    "    return description.replace(regexp, \"\")\n",
    "\n",
    "def remove_urls(description):\n",
    "    return remove_by_regex(description, regex.compile(r\"http\\S+\"))\n",
    "\n",
    "def remove_special_chars(description): \n",
    "    return description.apply(lambda desc: ''.join([c for c in desc if c not in specialChars]))\n",
    "\n",
    "def remove_usernames(description):\n",
    "    return remove_by_regex(description, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "\n",
    "def remove_numbers(description):\n",
    "    return remove_by_regex(description, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))\n",
    "\n",
    "def remove_emojis(description):\n",
    "    return description.apply(lambda desc: ''.join(c for c in desc if c not in emoji.UNICODE_EMOJI))\n",
    "\n",
    "def add_spaces(descriptions):\n",
    "    def add_spaces_int(description):\n",
    "        for char in space_chars:\n",
    "            description = description.replace(char, char + ' ')\n",
    "        return description\n",
    "    return descriptions.apply(lambda desc: add_spaces_int(desc))\n",
    "\n",
    "def leave_language_only(descriptions):\n",
    "    for f in [remove_urls, remove_emojis, add_spaces, remove_numbers, remove_usernames, remove_special_chars]:\n",
    "        descriptions = f(descriptions)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating df - a DataFrame with the original 'description' and 'points' variables and a new 'pureTextDescription' variable which will be used for Tokenization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(wine_reviews['description'])\n",
    "df['y'] = wine_reviews['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pureTextDescription'] = leave_language_only(df.description.str.lower())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words(\"english\") + nltk.corpus.stopwords.words(\"italian\") + nltk.corpus.stopwords.words(\"spanish\")\n",
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(df.pureTextDescription.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the dataset text using the pureTextDescription feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for desc in df.pureTextDescription for word in nltk.word_tokenize(desc) if word.lower() not in stopwords] # Words without stop words\n",
    "words_df = DataFrame(data = all_words, columns = ['word']).word.value_counts().reset_index()\n",
    "words_df.columns = ['word','wordCount']\n",
    "words_df['wordImportance'] = len(words_df) / words_df.wordCount / words_df.wordCount.max()\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.set_index('word').wordCount.head(20).iplot(kind = 'bar', title = 'Most frequent words in Corpus', yTitle = 'Count', xTitle = 'Word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of {} words, {} unique words\".format(len(all_words), len(words_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to reduce the corpus size more, we probably don't care about words that appear too little. Let's drop any word which have under 5 appearnces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Using words with 5 or more appearnces will reduce the corpus size to: {}\".format(sum(words_df.wordCount >= 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = words_df[words_df.wordCount >= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words (One-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_vocab = set(words_df.word)\n",
    "count_vect = CountVectorizer(vocabulary = clean_vocab, tokenizer=nltk.word_tokenize)\n",
    "clean_bow_counts = count_vect.fit_transform(df.pureTextDescription)\n",
    "clean_bow_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1].pureTextDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_bow_counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_dict = {v:k for k,v in count_vect.vocabulary_.items()}\n",
    "print(rev_dict[76])\n",
    "print(rev_dict[271])\n",
    "print(rev_dict[280])\n",
    "print(rev_dict[941])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_bow_counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing on the entire dataset (no split to test/train)\n",
    "(1) Cross-validation for searching the optimal regularization level<br>\n",
    "(2) Predicting 'points' using optimal regularization level on the entire dataset and evaluating prediction quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "search_grid = np.logspace(-2, 4, num=50, endpoint=True, base=10.0)\n",
    "mse_by_alpha = []\n",
    "for alpha in search_grid:\n",
    "    model = Ridge(alpha = alpha, tol=0.0001, max_iter=10000)\n",
    "    avg_score = cross_val_score(model, clean_bow_counts, y = df.y, cv = 10, scoring = 'neg_mean_squared_error').mean()\n",
    "    mse_by_alpha.append((alpha,abs(avg_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = DataFrame(mse_by_alpha, columns = ['alpha', 'mean_squared_error'])\n",
    "cv_results.mean_squared_error.iplot(title = 'BOW Counts - mean_squared_error as a function of Regularization rate (alpha)', xTitle = 'alpha', yTitle = 'mean_squared_error', width = 3, hline=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_alpha, min_mean_squared_error = cv_results.loc[cv_results.mean_squared_error.idxmin()]\n",
    "print(opt_alpha, min_mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting 'points' with the optimal model and evaluating prediction quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha = opt_alpha, tol=0.0001, max_iter=10000)\n",
    "model.fit(clean_bow_counts, df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_score'] = model.predict(clean_bow_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NLP_desc_stats = calc_prediction_quality(df, 'predicted_score','y')\n",
    "NLP_desc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared = pd.DataFrame([b1_stats, b2_stats, b3_stats, xgb_stats, NLP_desc_stats], index=['baseline_1', 'baseline_2','baseline_3','regression_by_xgb', 'NLP_desc_stats'])\n",
    "all_compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared.to_csv('data/all_models_compared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using both the text and other features\n",
    "#### Training and testing on the entire dataset (no split to test/train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features = encoded_features[['country', 'province', 'region_1', 'region_2', 'taster_name', 'variety', 'winery', 'price']]\n",
    "other_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "#other_features_spares_matrix = coo_matrix(other_features) # no need to apply coo_matrix as hstack converts to sparse matrix authomatically\n",
    "train_united_features = hstack((clean_bow_counts ,other_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "search_grid = np.logspace(-2, 4, num=50, endpoint=True, base=10.0)\n",
    "mse_by_alpha = []\n",
    "for alpha in search_grid:\n",
    "    model = Ridge(alpha = alpha, tol=0.0001, max_iter=10000)\n",
    "    avg_score = cross_val_score(model, train_united_features, y = df.y, cv = 10, scoring = 'neg_mean_squared_error').mean()\n",
    "    mse_by_alpha.append((alpha,abs(avg_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = DataFrame(mse_by_alpha, columns = ['alpha', 'mean_squared_error'])\n",
    "cv_results.mean_squared_error.iplot(title = 'BOW Counts - mean_squared_error as a function of the Regularization strength (alpha)', xTitle = 'alpha', yTitle = 'mean_squared_error', width = 3, hline=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_alpha, min_mean_squared_error = cv_results.loc[cv_results.mean_squared_error.idxmin()]\n",
    "print(opt_alpha, min_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha = opt_alpha, tol=0.0001, max_iter=10000)\n",
    "model.fit(train_united_features, df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_score_extended_NLP'] = model.predict(train_united_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_NLP_stats = calc_prediction_quality(df, 'predicted_score_extended_NLP','y')\n",
    "extended_NLP_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared = pd.DataFrame([b1_stats, b2_stats, b3_stats, xgb_stats, NLP_desc_stats, extended_NLP_stats], index=['baseline_1', 'baseline_2','baseline_3','regression_by_xgb', 'NLP_desc_stats', 'extended_NLP_stats'])\n",
    "all_compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more features to the text resulted in improvement MSE (as well as on the other metrics). This is the expected result as we added more features to the model. The improvement is not significant though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared.to_csv('data/all_models_compared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected network on the 'description' text feature only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Internal embedding layer + average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization and vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Dense, GlobalAveragePooling1D, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a good size for the vocabulary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.description.apply(lambda x: len(x.split(' '))).quantile([0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32000\n",
    "sequence_length = 72\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to integers. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    #standardize=lambda text: tf.strings.lower(text), # You can use your own normalization function here\n",
    "    standardize='lower_and_strip_punctuation', # Or you can use a pre-made normalization function\n",
    "    max_tokens=vocab_size,    \n",
    "    split='whitespace',\n",
    "    output_mode='int',\n",
    "    name = 'Text_processing',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the vocabulary of the TextVectorization layer based on the 'description' variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(train_data['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_description = train_data['description'].sample().iloc[0]\n",
    "print(sample_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer(sample_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer(sample_description).numpy()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in vectorize_layer(sample_description).numpy()[:20]:\n",
    "    print(f\"{token} ---> \",vectorize_layer.get_vocabulary()[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling (Sequential API):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total model parameters: 514,953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(164, activation='tanh', name='hidden_layer'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, name = 'output_layer')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_dtype=False, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(), loss='mean_absolute_error', metrics=['mean_squared_error','mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_col, target_col = 'description', 'points'\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_mean_squared_error',\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data[text_col],\n",
    "    train_data[target_col],\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    verbose=1,    \n",
    "    callbacks=[early_stopping_monitor],\n",
    "    validation_data = (test_data[text_col], test_data[target_col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['predicted_score_fully_connected_NN'] = model.predict(test_data[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_connected_NN_stats = calc_prediction_quality(test_data, 'predicted_score_fully_connected_NN', target_col)\n",
    "fully_connected_NN_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared = pd.DataFrame([b1_stats, b2_stats, b3_stats, xgb_stats, NLP_desc_stats, extended_NLP_stats, fully_connected_NN_stats], index=['baseline_1', 'baseline_2','baseline_3','regression_by_xgb', 'NLP_desc_stats', 'extended_NLP_stats', 'fully_connected_NN'])\n",
    "all_compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared.to_csv('data/all_models_compared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Fully connected NN with internal embedding and concatination (instead of average pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concatination is performed by reshaping of the outputs of the embedding layer to 1D vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling (Sequential API):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total model parameters: 701,257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape, Dense, Dropout\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "    Reshape((embedding_dim * sequence_length, ), name='concat_words'),\n",
    "    Dense(164, activation='tanh', name='hidden_layer'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='linear', name = 'output_layer')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(), loss='mean_absolute_error', metrics=['mean_squared_error','mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_dtype=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_col, target_col = 'description', 'points'\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_mean_squared_error',\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data[text_col],\n",
    "    train_data[target_col],\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    verbose=1,    \n",
    "    callbacks=[early_stopping_monitor],\n",
    "    validation_data = (test_data[text_col], test_data[target_col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['predicted_score_FC_NN_concatinated_words'] = model.predict(test_data[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_connected_NN_concatinated_words_stats = calc_prediction_quality(test_data, 'predicted_score_FC_NN_concatinated_words', target_col)\n",
    "fully_connected_NN_concatinated_words_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared = pd.DataFrame([b1_stats, b2_stats, b3_stats, xgb_stats, NLP_desc_stats, extended_NLP_stats, fully_connected_NN_stats, fully_connected_NN_concatinated_words_stats], index=['baseline_1', 'baseline_2','baseline_3','regression_by_xgb', 'NLP_desc_stats', 'extended_NLP_stats', 'fully_connected_NN', 'fully_connected_NN_concatinated_words'])\n",
    "all_compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By concatinating the embedding output vectors, instead of average pooling, we increased the number of parameters from 514,953 to 701,257 which increases the risk for overfitting. This may explain the degradation we see across the evaluation metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared.to_csv('data/all_models_compared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Fully connected network, using the external GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a dictionary with the pre-trained GloVe word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"glove.6B.50d.txt\"\n",
    "mypath = os.getcwd()\n",
    "path_to_glove_file = mypath + \"\\\\data\\\\\" + filename\n",
    "path_to_glove_file\n",
    "\n",
    "embeddings_index = {}   # the disctionary storing the GloVe words and their respective embedding vector  \n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index.get('drinking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a word embedding matrix with a word embedding vector for each word of the wine_reviews vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_glove_vocub = []\n",
    "i = 0\n",
    "for word in vectorize_layer.get_vocabulary():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        out_of_glove_vocub.append((i, vectorize_layer.get_vocabulary()[i]))     # record the words that do not have an embedding   \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_glove_vocub[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                            output_dim=50,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling (Sequential API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    embedding_layer,\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(164, activation='tanh', name='hidden_layer'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, name = 'output_layer')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(), loss='mean_absolute_error', metrics=['mean_squared_error','mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_col, target_col = 'description', 'points'\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_mean_squared_error',\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data[text_col],\n",
    "    train_data[target_col],\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    verbose=1,    \n",
    "    callbacks=[early_stopping_monitor],\n",
    "    validation_data = (test_data[text_col], test_data[target_col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['predicted_score_DNN_external_embedding_stats'] = model.predict(test_data[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_external_embedding_stats = calc_prediction_quality(test_data, 'predicted_score_DNN_external_embedding_stats', target_col)\n",
    "DNN_external_embedding_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared = pd.DataFrame([b1_stats, b2_stats, b3_stats, xgb_stats, NLP_desc_stats, extended_NLP_stats, fully_connected_NN_stats, fully_connected_NN_concatinated_words_stats, DNN_external_embedding_stats], index=['baseline_1', 'baseline_2','baseline_3','regression_by_xgb', 'NLP_desc_stats', 'extended_NLP_stats', 'fully_connected_NN', 'fully_connected_NN_concatinated_words', 'DNN_external_embedding_stats'])\n",
    "all_compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using the external GloVe embeddings yielded poorer performance across the evaluation metrics. This can be expected as the GloVe vocabulary does not contain many of the wineray domain-specific words (out-of-vocab) and therefore does not provide effective embeddings for the wine-reviews texts.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared.to_csv('data/all_models_compared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Fully connected network with LSTM layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the LSTM layer (with 164 units):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_layer = tf.keras.layers.LSTM(\n",
    "    164,\n",
    "    activation='tanh',\n",
    "    recurrent_activation='sigmoid',\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros',\n",
    "    unit_forget_bias=True,\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    time_major=False,\n",
    "    unroll=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling (Sequential API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "    LSTM_layer,\n",
    "    Dense(164, activation='relu', name='hidden_layer'),\n",
    "    #Dropout(0.7),\n",
    "    Dense(1, activation='linear', name = 'output_layer')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(), loss='mean_absolute_error', metrics=['mean_squared_error','mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_dtype=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_col, target_col = 'description', 'points'\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_mean_squared_error',\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data[text_col],\n",
    "    train_data[target_col],\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    verbose=1,    \n",
    "    callbacks=[early_stopping_monitor],\n",
    "    validation_data = (test_data[text_col], test_data[target_col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['predicted_score_LSTM'] = model.predict(test_data[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_stats = calc_prediction_quality(test_data, 'predicted_score_LSTM', target_col)\n",
    "LSTM_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared = pd.DataFrame([b1_stats, b2_stats, b3_stats, xgb_stats, NLP_desc_stats, extended_NLP_stats, fully_connected_NN_stats, fully_connected_NN_concatinated_words_stats, DNN_external_embedding_stats, LSTM_stats], index=['baseline_1', 'baseline_2','baseline_3','regression_by_xgb', 'NLP_desc_stats', 'extended_NLP_stats', 'fully_connected_NN', 'fully_connected_NN_concatinated_words', 'DNN_external_embedding_stats', 'LSTM'])\n",
    "all_compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared.to_csv('data/all_models_compared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus task: Using all features applying the Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two sets of inputs: InputA is the 'description' text feature, InputB is the rest of the features which we already labeled before.\n",
    "inputA = Input(shape=(1,), name=\"text input layer\", dtype=tf.string)\n",
    "inputB = Input(shape=(8,), name=\"other features input layer\")\n",
    "\n",
    "# The first branch operates on InputA: \n",
    "x = vectorize_layer(inputA)\n",
    "x = Embedding(vocab_size, embedding_dim, name=\"embedding\")(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dense(164, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(164, activation='relu')(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# The second branch opreates on InputB:\n",
    "y = Dense(164, activation=\"relu\")(inputB)\n",
    "y = Dropout(0.2)(y)\n",
    "y = Dense(164, activation=\"relu\")(y)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "# Combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output])\n",
    "\n",
    "# Apply a fully-connected layer and then a regression prediction on the combined outputs\n",
    "z = Dense(164, activation=\"relu\")(combined)\n",
    "z = Dense(1, activation=\"linear\")(z)\n",
    "\n",
    "# Define a model that will accept the inputs of the two branches and then output a single value\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(), loss='mean_absolute_error', metrics=['mean_squared_error','mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_dtype=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Train and Test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizing the Text train and test datasets\n",
    "trainTextX = train_data[text_col]\n",
    "testTextX = test_data[text_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizing the train and test datasets of the rest of features and which we already encoded before \n",
    "trainAttrX = train_encoded_features.loc[:,train_encoded_features.columns != 'points']\n",
    "testAttrX = test_encoded_features.loc[:,test_encoded_features.columns != 'points']\n",
    "testAttrX = testAttrX.drop(['xgb_clf_points_prediction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_col, target_col = 'description', 'points'\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_mean_squared_error',\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x= [trainTextX, trainAttrX],\n",
    "    y = train_data[target_col],\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    verbose=1,    \n",
    "    callbacks=[early_stopping_monitor],\n",
    "    validation_data = ([testTextX, testAttrX], test_data[target_col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Multiple_Inputs_Mixed_Data_NN_Functional_API'] = model.predict([testTextX, testAttrX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple_Inputs_Mixed_Data_NN_Functional_API_stats = calc_prediction_quality(test_data, 'Multiple_Inputs_Mixed_Data_NN_Functional_API', target_col)\n",
    "Multiple_Inputs_Mixed_Data_NN_Functional_API_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared = pd.DataFrame([b1_stats, b2_stats, b3_stats, xgb_stats, NLP_desc_stats, extended_NLP_stats, fully_connected_NN_stats, fully_connected_NN_concatinated_words_stats, DNN_external_embedding_stats, LSTM_stats, Multiple_Inputs_Mixed_Data_NN_Functional_API_stats], index=['baseline_1', 'baseline_2','baseline_3','regression_by_xgb', 'NLP_desc_stats', 'extended_NLP_stats', 'fully_connected_NN', 'fully_connected_NN_concatinated_words', 'DNN_external_embedding_stats', 'LSTM', 'Multiple_Inputs_Mixed_Data_NN_Functional_API'])\n",
    "all_compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network with multiple inputs and mixed data (text and other features) yielded a prety good result - similar to, yet slightly lower than, the classical NLP regression model with the other parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compared.to_csv('data/all_models_compared.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(all_compared, x=all_compared.index, y='MSE', \n",
    "        title=\"MSE of the different models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(all_compared, x=all_compared.index, y='MAE', \n",
    "        title=\"MAE of the different models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
